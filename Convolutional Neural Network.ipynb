{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File names import\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed433ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthy Controls (Eyes Open)\n",
    "H_EO_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EO\"\n",
    "file_names_H_EO = [f for f in listdir(H_EO_path) if isfile(join(H_EO_path, f))]\n",
    "\n",
    "# Healthy Controls (Eyes Closed)\n",
    "H_EC_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EC\"\n",
    "file_names_H_EC = [f for f in listdir(H_EC_path) if isfile(join(H_EC_path, f))]\n",
    "\n",
    "# Healthy Controls (Task)\n",
    "H_TASK_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/TASK\"\n",
    "file_names_H_TASK = [f for f in listdir(H_TASK_path) if isfile(join(H_TASK_path, f))]\n",
    "\n",
    "# MDD (Eyes Open)\n",
    "MDD_EO_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EO\"\n",
    "file_names_MDD_EO = [f for f in listdir(MDD_EO_path) if isfile(join(MDD_EO_path, f))]\n",
    "\n",
    "# MDD (Eyes Closed)\n",
    "MDD_EC_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EC\"\n",
    "file_names_MDD_EC = [f for f in listdir(MDD_EC_path) if isfile(join(MDD_EC_path, f))]\n",
    "\n",
    "# MDD (Task)\n",
    "MDD_TASK_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/TASK\"\n",
    "file_names_MDD_TASK = [f for f in listdir(MDD_TASK_path) if isfile(join(MDD_TASK_path, f))]\n",
    "\n",
    "# Create file names list\n",
    "file_names = [file_names_H_EO, file_names_H_EC, file_names_H_TASK, file_names_MDD_EO, file_names_MDD_EC, file_names_MDD_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b900045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "H_EO_data = []\n",
    "H_EC_data = []\n",
    "H_TASK_data = []\n",
    "MDD_EO_data = []\n",
    "MDD_EC_data = []\n",
    "MDD_TASK_data = []\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    for j in range(len(file_names[i])):\n",
    "        if i == 0:\n",
    "            file =  \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EO/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_EO_data.append(data.get_data())\n",
    "        if i == 1:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EC/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_EC_data.append(data.get_data())\n",
    "        if i == 2:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/TASK/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_TASK_data.append(data.get_data())\n",
    "        if i == 3:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EO/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_EO_data.append(data.get_data())\n",
    "        if i == 4:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EC/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_EC_data.append(data.get_data())\n",
    "        if i == 5:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/TASK/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_TASK_data.append(data.get_data())\n",
    "            if j == len(file_names[5]) - 1:\n",
    "                print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter function to filter EEG signals (due to electrical noise of the power grid)\n",
    "def filter(data):\n",
    "    # Input: numpy ndarray\n",
    "    \n",
    "    # Apply a high-pass filter at 0.5 Hz\n",
    "    mne.filter.filter_data(data, sfreq = 256, l_freq=0.5, h_freq=None)\n",
    "\n",
    "    # Apply a low-pass filter at 70 Hz\n",
    "    mne.filter.filter_data(data, sfreq = 256, l_freq=None, h_freq=70)\n",
    "\n",
    "    # Apply a notch filter at 50 Hz\n",
    "    mne.filter.notch_filter(data, Fs = 256, freqs=50)\n",
    "\n",
    "# We keep only the first 19 channels, as the other 3 are redundant\n",
    "# Then for each sample, we apply our filter \n",
    "for i in range(len(H_EO_data)):\n",
    "    H_EO_data[i] = H_EO_data[i][0:19]\n",
    "    filter(H_EO_data[i])\n",
    "for i in range(len(H_EC_data)):\n",
    "    H_EC_data[i] = H_EC_data[i][0:19]\n",
    "    filter(H_EC_data[i])\n",
    "for i in range(len(H_TASK_data)):\n",
    "    H_TASK_data[i] = H_TASK_data[i][0:19]\n",
    "    filter(H_TASK_data[i])\n",
    "for i in range(len(MDD_EO_data)):\n",
    "    MDD_EO_data[i] = MDD_EO_data[i][0:19]\n",
    "    filter(MDD_EO_data[i])\n",
    "for i in range(len(MDD_EC_data)):\n",
    "    MDD_EC_data[i] = MDD_EC_data[i][0:19]\n",
    "    filter(MDD_EC_data[i])\n",
    "for i in range(len(MDD_TASK_data)):\n",
    "    MDD_TASK_data[i] = MDD_TASK_data[i][0:19]\n",
    "    filter(MDD_TASK_data[i])\n",
    "    if i == len(MDD_TASK_data) - 1:\n",
    "        print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d617093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Artifacts (add later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase Lock Value\n",
    "import scipy\n",
    "def plv(e_i, e_j):\n",
    "    # e_i: Electrode i, a numpy array of shape (n,)\n",
    "    # e_j: Electrode j, a numpy array of shape (n,)\n",
    "    n = np.shape(e_i)[0]\n",
    "    \n",
    "    # Compute the phases\n",
    "    fft_i = scipy.fftpack.fft(e_i)\n",
    "    fft_j = scipy.fftpack.fft(e_j)\n",
    "    phase_i = np.angle(fft_i)\n",
    "    phase_j = np.angle(fft_j)\n",
    "    \n",
    "    # Phase differences\n",
    "    phase_diff = phase_i - phase_j\n",
    "    \n",
    "    # Sum terms\n",
    "    sum_terms = [np.exp(- (1j * (phase_diff[t]))) for t in range(n)]\n",
    "    phase_lock_value = (1/n) * np.abs(np.sum(sum_terms))\n",
    "    \n",
    "    return phase_lock_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ca6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase Lag Index\n",
    "def pli(e_i, e_j):\n",
    "    # e_i: Electrode i, a numpy array of shape n x 1\n",
    "    # e_j: Electrode j, a numpy array of shape n x 1\n",
    "    n = np.shape(e_i)[0]\n",
    "    \n",
    "    # Compute the phases\n",
    "    fft_i = scipy.fftpack.fft(e_i)\n",
    "    fft_j = scipy.fftpack.fft(e_j)\n",
    "    phase_i = np.angle(fft_i)\n",
    "    phase_j = np.angle(fft_j)\n",
    "    \n",
    "    # Phase differences\n",
    "    phase_diff = phase_i - phase_j\n",
    "    \n",
    "    # Phase lag index\n",
    "    sum_terms = [np.sign(np.imag(np.exp(1j * (phase_diff[t])))) for t in range(n)]\n",
    "    phase_lag_index = np.abs((1.000000/n) * sum(sum_terms))\n",
    "    \n",
    "    return phase_lag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor(e_i, e_j):\n",
    "    # e_i: Electrode i, a numpy array of shape n x 1\n",
    "    # e_j: Electrode j, a numpy array of shape n x 1\n",
    "    x = np.array([e_i, e_j])\n",
    "    return np.corrcoef(x)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sequence of weighted functional connectivity matrices\n",
    "def adj_matrix(A, start, stop, t_slice, method):\n",
    "    #A: Numpy array of shape 19 x (time points), representing the EEG data of a single patient\n",
    "    #start: The starting index time point\n",
    "    #stop: The stopping index time point\n",
    "    #t_slice: The time segment where we compute\n",
    "    #method: A string of either ['pli', 'plv', 'cor'], for Phase Lag Index, Phase Lock Value, or Correlation\n",
    "    \n",
    "    # Number of time slices we will compute\n",
    "    n = (stop - start) // t_slice\n",
    "    end = n * t_slice\n",
    "    \n",
    "    # Array storing the sequence of functional connectivity matrices\n",
    "    seq = np.zeros((n, 19, 19))\n",
    "    \n",
    "    # Phase Lag Index\n",
    "    if method == 'pli':\n",
    "        for k in range(0, end - t_slice + 1, t_slice):\n",
    "            A_temp = A[:19, k : k + t_slice]\n",
    "            matrix = np.zeros((19,19))\n",
    "            for i in range(19):\n",
    "                for j in range(19):\n",
    "                    matrix[i][j] = pli(A_temp[i], A_temp[j])\n",
    "            seq[k // t_slice] = matrix\n",
    "            \n",
    "    # Phase Lock Value       \n",
    "    if method == 'plv':\n",
    "        for k in range(0, end - t_slice + 1, t_slice):\n",
    "            A_temp = A[:19, k : k + t_slice]\n",
    "            matrix = np.zeros((19,19))\n",
    "            for i in range(19):\n",
    "                for j in range(19):\n",
    "                    matrix[i][j] = plv(A_temp[i], A_temp[j])\n",
    "            seq[k // t_slice] = matrix\n",
    "        \n",
    "    # Correlation\n",
    "    if method == 'cor':\n",
    "        for k in range(0, end - t_slice + 1, t_slice):\n",
    "            A_temp = A[:19, k : k + t_slice]\n",
    "            matrix = np.zeros((19,19))\n",
    "            for i in range(19):\n",
    "                for j in range(19):\n",
    "                    matrix[i][j] = cor(A_temp[i], A_temp[j])\n",
    "            seq[k // t_slice] = matrix\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train and test only the TASK data (ignoring EC and EO data for now)\n",
    "# Each EEG signal is varying in time length. We first find the EEG signal with least time\n",
    "M_1 = min([np.shape(H_TASK_data[i])[1] for i in range(len(H_TASK_data))])\n",
    "M_2 = min([np.shape(MDD_TASK_data[i])[1] for i in range(len(MDD_TASK_data))])\n",
    "\n",
    "# Round down to nearest order of 10000\n",
    "M = min(M_1, M_2) - 3856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "start = 0\n",
    "stop = M\n",
    "t_slice = 10000\n",
    "method = 'plv'\n",
    "\n",
    "# Make seq of graphs for each patient\n",
    "MDD_TASK_graphs = []\n",
    "H_TASK_graphs = []\n",
    "\n",
    "for x in MDD_TASK_data:\n",
    "    patient_seq = adj_matrix(x, start, stop, t_slice, method)\n",
    "    patient_id = 1\n",
    "    MDD_TASK_graphs.append([patient_seq, patient_id])\n",
    "\n",
    "for x in H_TASK_data:\n",
    "    patient_seq = adj_matrix(x, start, stop, t_slice, method)\n",
    "    patient_id = 0\n",
    "    H_TASK_graphs.append([patient_seq, patient_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lists to get our dataset\n",
    "# This is a dataset of size n = 61, one example for each patient\n",
    "# 15 graphs per patient over different time points\n",
    "TASK_graphs = MDD_TASK_graphs + H_TASK_graphs\n",
    "\n",
    "# Shuffle data\n",
    "import random\n",
    "random.shuffle(TASK_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98aabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 15 graphs per patient into one dataset\n",
    "# Now our dataset is of size n = 915, which are all graphs mixed together, \n",
    "# with their corresponding patient_id label (1 or 0, representing MDD or healthy)\n",
    "TASK_graphs_mixed = []\n",
    "\n",
    "for i in range(len(TASK_graphs)):\n",
    "    patient_id = TASK_graphs[i][1]\n",
    "    for j in range(15):\n",
    "        graph = TASK_graphs[i][0][j]\n",
    "        TASK_graphs_mixed.append([graph, patient_id])\n",
    "\n",
    "# Shuffle data\n",
    "import random\n",
    "random.shuffle(TASK_graphs_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ffd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\t\t\t\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate things into an X and Y arrays, representing inputs and labels\n",
    "n = len(TASK_graphs_mixed)\n",
    "X = np.zeros((n, 19, 19))\n",
    "Y = np.zeros((n, 1))\n",
    "\n",
    "for i in range(n):\n",
    "    X[i] = TASK_graphs_mixed[i][0]\n",
    "    Y[i] = TASK_graphs_mixed[i][1]\n",
    "\n",
    "# Translate the above to torch tensors\n",
    "X_tensor = torch.Tensor(X)\n",
    "Y_tensor = torch.Tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNs ONLY\n",
    "# Reshape each input in X as shape (1,19,19) not (19,19)\n",
    "# We do this because this is how convolutional layers want their dimensions\n",
    "n = len(TASK_graphs_mixed)\n",
    "X = np.zeros((n, 1, 19, 19))\n",
    "Y = np.zeros((n, 1))\n",
    "\n",
    "for i in range(n):\n",
    "    X[i] = np.reshape(TASK_graphs_mixed[i][0], (1,19,19))\n",
    "    Y[i] = TASK_graphs_mixed[i][1]\n",
    "\n",
    "# Translate the above to torch tensors\n",
    "X_tensor = torch.Tensor(X)\n",
    "Y_tensor = torch.Tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, test set, and their dataloaders\n",
    "trainset = TensorDataset(X_tensor[0:715], Y_tensor[0:715])\n",
    "testset = TensorDataset(X_tensor[715:915], Y_tensor[715:915] )\n",
    "trainloader = DataLoader(trainset)\n",
    "testloader = DataLoader(testset)\n",
    "classes = (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten\n",
    "        x = x.view(-1, 16 * 3 * 3)\n",
    "        # Classification layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00530cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "net = Net()\n",
    "\n",
    "# Loss function (binary cross entropy)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# # Optimizer (SGD with momentum)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Optimizer: Adams\n",
    "optimizer = optim.Adam(params = net.parameters(), lr = 0.0001)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(80):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# save trained model\n",
    "PATH = 'model_1.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\t\t\t\t\t\t\t  \n",
    "# Accuracy on the full training set (700 graphs)\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in trainloader:\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs).data\n",
    "    \n",
    "    # Probability threshold of 0.5\n",
    "    if outputs > 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    total += 1\n",
    "    if prediction == labels:\n",
    "        correct += 1\n",
    "print('Training accuracy of the network on the 200 graphs: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd376ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\t\t\t\t\t\t\t  \n",
    "# Accuracy on the full test set (200 graphs)\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs).data\n",
    "    \n",
    "    # Compute the prediction from the sigmoid probability\n",
    "    # We set the probability threshold of 0.5\n",
    "    # This threshold works because we have nearly the same number of MDD patients (n = 33) as Healthy patients (n = 28)\n",
    "    if outputs > 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    total += 1\n",
    "    if prediction == labels:\n",
    "        correct += 1\n",
    "print('Test accuracy of the network on the 200 graphs: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
