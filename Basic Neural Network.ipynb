{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4db5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File names import\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthy Controls (Eyes Open)\n",
    "H_EO_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EO\"\n",
    "file_names_H_EO = [f for f in listdir(H_EO_path) if isfile(join(H_EO_path, f))]\n",
    "\n",
    "# Healthy Controls (Eyes Closed)\n",
    "H_EC_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EC\"\n",
    "file_names_H_EC = [f for f in listdir(H_EC_path) if isfile(join(H_EC_path, f))]\n",
    "\n",
    "# Healthy Controls (Task)\n",
    "H_TASK_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/TASK\"\n",
    "file_names_H_TASK = [f for f in listdir(H_TASK_path) if isfile(join(H_TASK_path, f))]\n",
    "\n",
    "# MDD (Eyes Open)\n",
    "MDD_EO_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EO\"\n",
    "file_names_MDD_EO = [f for f in listdir(MDD_EO_path) if isfile(join(MDD_EO_path, f))]\n",
    "\n",
    "# MDD (Eyes Closed)\n",
    "MDD_EC_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EC\"\n",
    "file_names_MDD_EC = [f for f in listdir(MDD_EC_path) if isfile(join(MDD_EC_path, f))]\n",
    "\n",
    "# MDD (Task)\n",
    "MDD_TASK_path = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/TASK\"\n",
    "file_names_MDD_TASK = [f for f in listdir(MDD_TASK_path) if isfile(join(MDD_TASK_path, f))]\n",
    "\n",
    "# Create file names list\n",
    "file_names = [file_names_H_EO, file_names_H_EC, file_names_H_TASK, file_names_MDD_EO, file_names_MDD_EC, file_names_MDD_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4be90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "H_EO_data = []\n",
    "H_EC_data = []\n",
    "H_TASK_data = []\n",
    "MDD_EO_data = []\n",
    "MDD_EC_data = []\n",
    "MDD_TASK_data = []\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    for j in range(len(file_names[i])):\n",
    "        if i == 0:\n",
    "            file =  \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EO/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_EO_data.append(data.get_data())\n",
    "        if i == 1:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/EC/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_EC_data.append(data.get_data())\n",
    "        if i == 2:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/H/TASK/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            H_TASK_data.append(data.get_data())\n",
    "        if i == 3:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EO/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_EO_data.append(data.get_data())\n",
    "        if i == 4:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/EC/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_EC_data.append(data.get_data())\n",
    "        if i == 5:\n",
    "            file = \"C:/Users/xmoot/Desktop/Jupyter/Mathematical Biology/Final Project/HUMS Dataset/MDD/TASK/\" + file_names[i][j]\n",
    "            data = mne.io.read_raw_edf(file)\n",
    "            MDD_TASK_data.append(data.get_data())\n",
    "            if j == len(file_names[5]) - 1:\n",
    "                print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48345745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter function to filter EEG signals (due to electrical noise of the power grid)\n",
    "def filter(data):\n",
    "    # Input: numpy ndarray\n",
    "    \n",
    "    # Apply a high-pass filter at 0.5 Hz\n",
    "    mne.filter.filter_data(data, sfreq = 256, l_freq=0.5, h_freq=None)\n",
    "    # Apply a low-pass filter at 70 Hz\n",
    "    mne.filter.filter_data(data, sfreq = 256, l_freq=None, h_freq=70)\n",
    "    # Apply a notch filter at 50 Hz\n",
    "    mne.filter.notch_filter(data, Fs = 256, freqs=50)\n",
    "\n",
    "# We keep only the first 19 channels, as the other 3 are redundant\n",
    "# Then for each sample, we apply our filter \n",
    "for i in range(len(H_EO_data)):\n",
    "    H_EO_data[i] = H_EO_data[i][0:19]\n",
    "    filter(H_EO_data[i])\n",
    "for i in range(len(H_EC_data)):\n",
    "    H_EC_data[i] = H_EC_data[i][0:19]\n",
    "    filter(H_EC_data[i])\n",
    "for i in range(len(H_TASK_data)):\n",
    "    H_TASK_data[i] = H_TASK_data[i][0:19]\n",
    "    filter(H_TASK_data[i])\n",
    "for i in range(len(MDD_EO_data)):\n",
    "    MDD_EO_data[i] = MDD_EO_data[i][0:19]\n",
    "    filter(MDD_EO_data[i])\n",
    "for i in range(len(MDD_EC_data)):\n",
    "    MDD_EC_data[i] = MDD_EC_data[i][0:19]\n",
    "    filter(MDD_EC_data[i])\n",
    "for i in range(len(MDD_TASK_data)):\n",
    "    MDD_TASK_data[i] = MDD_TASK_data[i][0:19]\n",
    "    filter(MDD_TASK_data[i])\n",
    "    if i == len(MDD_TASK_data) - 1:\n",
    "        print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a666bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this\n",
    "temp1 = H_TASK_data\n",
    "temp2 = MDD_TASK_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload\n",
    "H_TASK_data = temp1\n",
    "MDD_TASK_data = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train on the TASK data (ignoring EC and EO data for now)\n",
    "# Each EEG signal is varying in time length. We first find the EEG signal with least time\n",
    "M_1 = min([np.shape(H_TASK_data[i])[1] for i in range(len(H_TASK_data))])\n",
    "M_2 = min([np.shape(MDD_TASK_data[i])[1] for i in range(len(MDD_TASK_data))])\n",
    "\n",
    "# Round down to nearest order of 10000\n",
    "M = min(M_1, M_2) - 3856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the end of each EEG signal down to this minimum time length M\n",
    "# Split into signal lengths of 10000 discrete time points each\n",
    "# Take the average across each time point\n",
    "for i in range(len(H_TASK_data)):\n",
    "    length = np.shape(H_TASK_data[i])[1]\n",
    "    if length > M:\n",
    "        H_TASK_data[i] = np.delete(H_TASK_data[i], list(range(M, length)), 1)\n",
    "        H_TASK_data[i] = np.split(H_TASK_data[i], 15, axis = 1)\n",
    "\n",
    "for i in range(len(MDD_TASK_data)):\n",
    "    length = np.shape(MDD_TASK_data[i])[1]\n",
    "    if length > M:\n",
    "        MDD_TASK_data[i] = np.delete(MDD_TASK_data[i], list(range(M, length)), 1)\n",
    "        MDD_TASK_data[i] = np.split(MDD_TASK_data[i], 15, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction\n",
    "# Since there are too many time points, we must reduce the data in meaningful manner to construct our features\n",
    "# We begin by averaging the data across each 10000 discrete time points \n",
    "H_TASK_avg = np.zeros((28,15,19))\n",
    "MDD_TASK_avg = np.zeros((33,15,19))\n",
    "\n",
    "for i in range(len(H_TASK_data)):\n",
    "    for j in range(15):\n",
    "        for k in range(19):\n",
    "            H_TASK_avg[i][j][k] = np.mean(H_TASK_data[i][j][k])\n",
    "\n",
    "for i in range(len(MDD_TASK_data)):\n",
    "    for j in range(15):\n",
    "        for k in range(19):\n",
    "            MDD_TASK_avg[i][j][k] = np.mean(MDD_TASK_data[i][j][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd823685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our H_TASK_data samples and MDD_TASK_data samples into an array\n",
    "# 0 corresponds to Healthy Control and 1 corresponds to MDD\n",
    "# We flatten our matrices according to 19 electrodes * 15 time points = 285\n",
    "total_TASK_data = [[(H_TASK_avg[i].flatten()).reshape(285,1), 0] for i in range(len(H_TASK_data))]\n",
    "for i in range(len(MDD_TASK_data)):\n",
    "      total_TASK_data.append([(MDD_TASK_avg[i].flatten()).reshape(285,1), 1])\n",
    "\n",
    "# Shuffle our samples, otherwise this will interfere with training\n",
    "import random\n",
    "random.shuffle(total_TASK_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9202b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now split the data into our input array X, and output array Y\n",
    "# We also to flatten each of our inputs from total_TASK_data into vectors, so we get the input matrix X = [x^(1) x^(2) . . . x^(m)]\n",
    "# We have n = 285 features per input vector, so our matrix will be of size 285 x 61\n",
    "input_vectors_TASK = [total_TASK_data[i][0] for i in range(len(total_TASK_data))]\n",
    "X = np.dstack(input_vectors_TASK).squeeze()\n",
    "\n",
    "output_vectors_TASK = [total_TASK_data[i][1] for i in range(len(total_TASK_data))]\n",
    "Y = np.dstack(output_vectors_TASK).squeeze()\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b929fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\t\t\t\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_tensor = torch.Tensor(X.T)\n",
    "Y_tensor = torch.Tensor(Y)\n",
    "\n",
    "# Datasets\n",
    "# Load training set, test set, and their dataloaders\n",
    "trainset = TensorDataset(X_tensor[0:40], Y_tensor[0:40])\n",
    "testset = TensorDataset(X_tensor[40:61], Y_tensor[40:61] )\n",
    "trainloader = DataLoader(trainset)\n",
    "testloader = DataLoader(testset)\n",
    "classes = (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0672527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (7 layer DNN)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(285, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, 256)\n",
    "        self.fc6 = nn.Linear(256, 128)\n",
    "        self.fc7 = nn.Linear(128, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.sigmoid(self.fc8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5750c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "net = Net()\n",
    "\n",
    "# Loss function (binary cross entropy)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer (SGD with momentum)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # Optimizer: Adams\n",
    "# optimizer = optim.Adam(params = net.parameters(), lr = 0.0001)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# save trained model\n",
    "PATH = '7_DNN_1.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beda950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "# Load trained network\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\t\t\t\t\t\t\t  \n",
    "# Accuracy on the full training set (700 graphs)\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in trainloader:\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs).data\n",
    "    \n",
    "    # Probability threshold of 0.5\n",
    "    if outputs > 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    total += 1\n",
    "    if prediction == labels:\n",
    "        correct += 1\n",
    "print('Training accuracy of the network on the 40 patients: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "# Load trained network\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\t\t\t\t\t\t\t  \n",
    "# Accuracy on the full test set (200 graphs)\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs).data\n",
    "    \n",
    "    # Compute the prediction from the sigmoid probability\n",
    "    # We set the probability threshold of 0.5\n",
    "    # This threshold works because we have nearly the same number of MDD patients (n = 33) as Healthy patients (n = 28)\n",
    "    if outputs > 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    total += 1\n",
    "    if prediction == labels:\n",
    "        correct += 1\n",
    "print('Test accuracy of the network on the 20 patients: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
